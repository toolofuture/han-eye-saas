

# ğŸ¤– Han.Eye - Reinforcement Learning from Demonstration (RLfD)

**ë…¼ë¬¸ ê¸°ë°˜ êµ¬í˜„**: "Robust quantum control using reinforcement learning from demonstration" (Nature npj Quantum Information, 2025)

## ğŸ“š ê°œìš”

Han.EyeëŠ” ì´ì œ **ì§„ì§œ ê°•í™”í•™ìŠµ(Reinforcement Learning)**ì„ ì‚¬ìš©í•˜ì—¬ ìê°€ê°œì„ ë©ë‹ˆë‹¤!

### ê¸°ì¡´ vs ìƒˆë¡œìš´ ë°©ì‹

| í•­ëª© | ê¸°ì¡´ (íœ´ë¦¬ìŠ¤í‹±) | ìƒˆë¡œìš´ (RLfD) |
|------|----------------|---------------|
| **íŒŒë¼ë¯¸í„°** | ê³ ì • (threshold=0.7) | ë™ì  í•™ìŠµ |
| **í•™ìŠµ** | ì—†ìŒ | SAC ê°•í™”í•™ìŠµ |
| **ê°œì„ ** | ìˆ˜ë™ ì¡°ì • | ìë™ ìµœì í™” |
| **ë°ì´í„° í™œìš©** | ì €ì¥ë§Œ | í•™ìŠµì— ì‚¬ìš© |

## ğŸ—ï¸ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Demonstration (ì´ˆê¸° ì§€ì‹)            â”‚
â”‚     - íœ´ë¦¬ìŠ¤í‹± íŒŒë¼ë¯¸í„°                  â”‚
â”‚     - ì‚¬ìš©ì í”¼ë“œë°±                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Pre-training (ëª¨ë°© í•™ìŠµ)            â”‚
â”‚     - Behavioral Cloning                â”‚
â”‚     - Demonstration Replay Buffer       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. RL Training (ê°•í™”í•™ìŠµ)              â”‚
â”‚     - Algorithm: SAC                    â”‚
â”‚     - Environment: Artwork Analysis     â”‚
â”‚     - Reward: User Feedback             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Optimized Model (ìµœì í™”ëœ ëª¨ë¸)     â”‚
â”‚     - ìµœì  threshold ìë™ ì„ íƒ          â”‚
â”‚     - ìµœì  weights ìë™ ì¡°ì •            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
cd /Users/kangsikseo/Downloads/han-eye-saas

# RL ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install stable-baselines3 gymnasium
```

### 2. ë°ì´í„° ìˆ˜ì§‘

ì›¹ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”:
- âœ… **ì •í™•í•¨**: AI íŒë‹¨ì´ ë§ì•˜ì„ ë•Œ
- âŒ **ë¶€ì •í™•í•¨**: AI íŒë‹¨ì´ í‹€ë ¸ì„ ë•Œ

**ìµœì†Œ 5ê°œ ì´ìƒì˜ í”¼ë“œë°±ì´ í•„ìš”í•©ë‹ˆë‹¤.**

### 3. RL ëª¨ë¸ í•™ìŠµ

```bash
python scripts/train_rl_agent.py
```

í•™ìŠµ ê³¼ì •:
```
ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì¤‘...
âœ… ìˆ˜ì§‘ëœ í•™ìŠµ ë°ì´í„°: 10ê°œ
   - ì§„í’ˆ: 6ê°œ
   - ìœ„ì‘: 4ê°œ

ğŸ¤– Reinforcement Learning from Demonstration (RLfD)
============================================================

ğŸ“¦ Step 1: í™˜ê²½ ìƒì„±...
   âœ… Batch environment with 10 images

ğŸ§  Step 2: ì—ì´ì „íŠ¸ ìƒì„±...
   âœ… RLfD Agent initialized

ğŸ¯ Step 3: Demonstration ìƒì„±...
   âœ… Total demonstrations: 13

ğŸ“ Step 4: Pre-training (Behavioral Cloning)...
   âœ… Pre-training completed

ğŸš€ Step 5: Reinforcement Learning...
   Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000

ğŸ“Š Step 6: ëª¨ë¸ í‰ê°€...
   Mean Reward: 0.85

ğŸ’¾ Step 7: ëª¨ë¸ ì €ì¥...
   âœ… Model saved

âœ… RLfD Training Complete!
```

### 4. RL ëª¨ë¸ ì‚¬ìš©

í•™ìŠµëœ ëª¨ë¸ì€ ìë™ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤:

```python
# app/utils/rl_detector.pyê°€ ìë™ìœ¼ë¡œ ë¡œë“œ
from app.utils.rl_detector import RLAnomalyDetector

detector = RLAnomalyDetector()
result = detector.analyze(image_path)

# RL ëª¨ë¸ì´ ìµœì  íŒŒë¼ë¯¸í„° ì„ íƒ
print(f"RL threshold: {result['rl_threshold']}")  # ì˜ˆ: 0.73
print(f"RL weights: {result['rl_weights']}")      # ì˜ˆ: [0.3, 0.3, 0.2, 0.2]
```

## ğŸ§  ê°•í™”í•™ìŠµ êµ¬ì¡°

### Environment (í™˜ê²½)

```python
class ArtworkAuthEnv:
    # State: ì´ë¯¸ì§€ íŠ¹ì§• [texture, edge, color, noise]
    observation_space = Box(low=0, high=1, shape=(4,))
    
    # Action: ì´ìƒíƒì§€ íŒŒë¼ë¯¸í„° [threshold, weights...]
    action_space = Box(low=0, high=1, shape=(5,))
    
    # Reward: ì‚¬ìš©ì í”¼ë“œë°±
    # +1: ì •í™•í•œ íŒë‹¨
    # -1: í‹€ë¦° íŒë‹¨
```

### Agent (ì—ì´ì „íŠ¸)

```python
# SAC (Soft Actor-Critic) ì•Œê³ ë¦¬ì¦˜
- Policy Network: ìµœì  action ì„ íƒ
- Q-Networks: actionì˜ ê°€ì¹˜ í‰ê°€
- Entropy Regularization: íƒìƒ‰ê³¼ í™œìš© ê· í˜•
```

### Demonstration

```python
# 1. íœ´ë¦¬ìŠ¤í‹± íŒŒë¼ë¯¸í„°
{
    'threshold': 0.7,
    'weights': [0.25, 0.25, 0.25, 0.25]
}

# 2. ì‚¬ìš©ì í”¼ë“œë°±
{
    'image': 'artwork.jpg',
    'ground_truth': True,  # ì§„í’ˆ
    'feedback': 'correct'
}
```

## ğŸ“Š ì„±ëŠ¥ ê°œì„  ì˜ˆì‹œ

```
Initial (Heuristic):
  Threshold: 0.70 (fixed)
  Accuracy: 50%

After RLfD (100 samples):
  Threshold: 0.73 (learned)
  Accuracy: 65% (+15%p)

After RLfD (1000 samples):
  Threshold: 0.68 (learned)
  Accuracy: 78% (+28%p)
```

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### í•™ìŠµ íŒŒë¼ë¯¸í„° ì¡°ì •

```python
# scripts/train_rl_agent.py

train_with_rlfd(
    pretrain_epochs=100,      # Pre-training ë°˜ë³µ íšŸìˆ˜
    train_timesteps=10000,    # RL í•™ìŠµ ìŠ¤í…
)
```

### ì•Œê³ ë¦¬ì¦˜ ë³€ê²½

```python
# PPO ì‚¬ìš©
from stable_baselines3 import PPO

model = PPO(
    "MlpPolicy",
    env,
    verbose=1,
    learning_rate=3e-4,
)
```

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

### TensorBoard ì‚¬ìš©

```bash
# í•™ìŠµ ê³¼ì • ì‹œê°í™”
tensorboard --logdir logs/

# ë¸Œë¼ìš°ì €ì—ì„œ
http://localhost:6006
```

### ì„±ëŠ¥ í‰ê°€

```python
from app.utils.rl_agent import RLfDAgent

agent = RLfDAgent(env)
agent.load('models/rl_artwork_agent')

metrics = agent.evaluate(n_eval_episodes=10)
print(f"Mean Reward: {metrics['mean_reward']}")
```

## ğŸ¯ ëª¨ë²” ì‚¬ë¡€

### 1. ì¶©ë¶„í•œ ë°ì´í„° ìˆ˜ì§‘

- âœ… ìµœì†Œ 10ê°œ ì´ìƒì˜ í”¼ë“œë°±
- âœ… ì§„í’ˆ/ìœ„ì‘ ê· í˜• (50:50)
- âœ… ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ ì‘í’ˆ

### 2. ì •ê¸°ì ì¸ ì¬í•™ìŠµ

```bash
# ìƒˆë¡œìš´ í”¼ë“œë°±ì´ ìŒ“ì´ë©´ ì¬í•™ìŠµ
python scripts/train_rl_agent.py
```

### 3. A/B í…ŒìŠ¤íŠ¸

```python
# íœ´ë¦¬ìŠ¤í‹± vs RL ë¹„êµ
heuristic_detector = AnomalyDetector()
rl_detector = RLAnomalyDetector()

# ë™ì¼ ì´ë¯¸ì§€ë¡œ ë¹„êµ
result1 = heuristic_detector.analyze(image)
result2 = rl_detector.analyze(image)
```

## ğŸ› ë¬¸ì œ í•´ê²°

### "stable-baselines3 not installed"

```bash
pip install stable-baselines3 gymnasium
```

### "No training data available"

ì›¹ì—ì„œ í”¼ë“œë°±ì„ ë” ìˆ˜ì§‘í•˜ì„¸ìš” (ìµœì†Œ 5ê°œ)

### "Training failed"

```bash
# ë¡œê·¸ í™•ì¸
tail -f logs/rl_training.log

# í™˜ê²½ í…ŒìŠ¤íŠ¸
python -c "from app.utils.rl_environment import ArtworkAuthEnv; print('OK')"
```

## ğŸ“š ì°¸ê³  ìë£Œ

- **ë…¼ë¬¸**: [Robust quantum control using reinforcement learning from demonstration](https://doi.org/10.1038/s41534-025-01065-2)
- **Stable-Baselines3**: https://stable-baselines3.readthedocs.io/
- **Gymnasium**: https://gymnasium.farama.org/

## ğŸ‰ ê²°ê³¼

RLfDë¥¼ ì‚¬ìš©í•˜ë©´:
- âœ… **ìë™ íŒŒë¼ë¯¸í„° ìµœì í™”**: ìˆ˜ë™ ì¡°ì • ë¶ˆí•„ìš”
- âœ… **ì§€ì†ì  ê°œì„ **: í”¼ë“œë°±ì´ ìŒ“ì¼ìˆ˜ë¡ ì •í™•ë„ í–¥ìƒ
- âœ… **ìƒ˜í”Œ íš¨ìœ¨ì„±**: ì ì€ ë°ì´í„°ë¡œë„ í•™ìŠµ ê°€ëŠ¥
- âœ… **ì•ˆì •ì  í•™ìŠµ**: Demonstrationìœ¼ë¡œ ì´ˆê¸°í™”

**50% â†’ 80% ì •í™•ë„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤!** ğŸš€

