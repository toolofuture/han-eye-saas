"""
Reinforcement Learning from Demonstration (RLfD) Agent
Inspired by the quantum control paper
"""

import numpy as np
from typing import Dict, Any, List, Optional, Tuple
import os
import pickle


class RLfDAgent:
    """
    RLfD Agent for Artwork Authentication
    
    Uses SAC (Soft Actor-Critic) with demonstration pre-training
    """
    
    def __init__(self, env, model_path: Optional[str] = None):
        """
        Args:
            env: Gymnasium environment
            model_path: Path to save/load model
        """
        self.env = env
        self.model_path = model_path or 'models/rl_artwork_agent'
        self.model = None
        self.demonstrations = []
        
    def add_demonstration(self, state: np.ndarray, action: np.ndarray, 
                         reward: float, next_state: np.ndarray):
        """
        Add demonstration data
        
        Demonstrations can come from:
        1. Current heuristic parameters
        2. Expert human labels
        3. GRAPE-like optimization methods
        """
        self.demonstrations.append({
            'state': state,
            'action': action,
            'reward': reward,
            'next_state': next_state
        })
    
    def load_demonstrations_from_heuristics(self):
        """
        Load demonstrations from current heuristic method
        
        í˜„ì¬ íœ´ë¦¬ìŠ¤í‹± íŒŒë¼ë¯¸í„°ë¥¼ demonstrationìœ¼ë¡œ ì‚¬ìš©
        """
        # Default heuristic parameters
        heuristic_action = np.array([
            0.7,    # threshold
            0.25,   # texture_weight
            0.25,   # edge_weight
            0.25,   # color_weight
            0.25    # noise_weight
        ], dtype=np.float32)
        
        # Generate demonstration from heuristic
        print("ğŸ¯ Generating demonstrations from heuristic parameters...")
        
        state, _ = self.env.reset()
        next_state, reward, done, truncated, info = self.env.step(heuristic_action)
        
        self.add_demonstration(state, heuristic_action, reward, next_state)
        
        print(f"âœ… Added demonstration: reward={reward:.2f}, prediction={info['prediction']}")
        
        return len(self.demonstrations)
    
    def load_demonstrations_from_feedback(self, db_session):
        """
        Load demonstrations from user feedback
        
        ì‚¬ìš©ì í”¼ë“œë°±ì´ ìˆëŠ” ë¶„ì„ë“¤ì„ demonstrationìœ¼ë¡œ ì‚¬ìš©
        """
        from app.models import Analysis
        
        print("ğŸ“Š Loading demonstrations from user feedback...")
        
        # Get analyses with feedback
        analyses = db_session.query(Analysis).filter(
            Analysis.user_feedback.in_(['correct', 'incorrect']),
            Analysis.anomaly_score.isnot(None)
        ).limit(100).all()
        
        demo_count = 0
        for analysis in analyses:
            try:
                # Reconstruct state
                result = analysis.get_analysis_result_dict()
                style = analysis.get_style_analysis_dict()
                tech = analysis.get_technique_analysis_dict()
                
                # Create pseudo-state (simplified)
                state = np.array([
                    analysis.anomaly_score,
                    analysis.confidence_score,
                    0.5,  # placeholder
                    0.5   # placeholder
                ], dtype=np.float32)
                
                # Reconstruct action (estimated from analysis)
                action = np.array([
                    0.7,  # threshold (default)
                    0.25, 0.25, 0.25, 0.25  # weights (default)
                ], dtype=np.float32)
                
                # Ground truth from feedback
                if analysis.user_feedback == 'correct':
                    ground_truth = analysis.is_authentic
                else:
                    ground_truth = not analysis.is_authentic
                
                # Reward: correct prediction = +1, incorrect = -1
                prediction = analysis.is_authentic
                reward = 1.0 if prediction == ground_truth else -1.0
                
                self.add_demonstration(state, action, reward, state)
                demo_count += 1
                
            except Exception as e:
                print(f"Warning: Failed to load demonstration: {e}")
                continue
        
        print(f"âœ… Loaded {demo_count} demonstrations from feedback")
        return demo_count
    
    def pretrain_with_demonstrations(self, epochs: int = 100):
        """
        Pre-train agent with demonstrations using behavioral cloning
        
        ë…¼ë¬¸ì˜ RLfDì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•:
        1. Demonstrationìœ¼ë¡œ supervised learning
        2. ì´í›„ RLë¡œ fine-tuning
        """
        if len(self.demonstrations) == 0:
            print("âš ï¸  No demonstrations available for pre-training")
            return
        
        print(f"ğŸ“ Pre-training with {len(self.demonstrations)} demonstrations...")
        
        try:
            from stable_baselines3 import SAC
            from stable_baselines3.common.noise import NormalActionNoise
            
            # Initialize SAC agent
            n_actions = self.env.action_space.shape[0]
            action_noise = NormalActionNoise(
                mean=np.zeros(n_actions),
                sigma=0.1 * np.ones(n_actions)
            )
            
            self.model = SAC(
                "MlpPolicy",
                self.env,
                action_noise=action_noise,
                verbose=1,
                learning_rate=3e-4,
                buffer_size=100000,
                batch_size=64,
                tau=0.005,
                gamma=0.99
            )
            
            # Pre-fill replay buffer with demonstrations
            for demo in self.demonstrations:
                self.model.replay_buffer.add(
                    obs=demo['state'],
                    next_obs=demo['next_state'],
                    action=demo['action'],
                    reward=np.array([demo['reward']]),
                    done=np.array([True]),
                    infos=[{}]
                )
            
            print("âœ… Pre-training completed with demonstration replay buffer")
            
        except ImportError:
            print("âš ï¸  stable-baselines3 not installed. Using behavioral cloning fallback...")
            self._behavioral_cloning_pretrain(epochs)
    
    def _behavioral_cloning_pretrain(self, epochs: int):
        """
        Fallback: Simple behavioral cloning without RL library
        """
        print("ğŸ“š Behavioral cloning pre-training...")
        
        # Extract states and actions
        states = np.array([d['state'] for d in self.demonstrations])
        actions = np.array([d['action'] for d in self.demonstrations])
        
        # Simple supervised learning would go here
        # For MVP, we just store the demonstrations
        
        print(f"âœ… Stored {len(self.demonstrations)} demonstration pairs")
    
    def train(self, total_timesteps: int = 10000):
        """
        Train agent with reinforcement learning
        
        ë…¼ë¬¸ì˜ RLfD ë°©ì‹:
        1. Pre-trained modelë¡œ ì‹œì‘
        2. Environmentì™€ interactioní•˜ë©° í•™ìŠµ
        3. Demonstration replay buffer ê³„ì† ì‚¬ìš©
        """
        if self.model is None:
            print("âš ï¸  Model not initialized. Call pretrain_with_demonstrations() first")
            return
        
        print(f"ğŸš€ Training RL agent for {total_timesteps} timesteps...")
        
        try:
            self.model.learn(
                total_timesteps=total_timesteps,
                log_interval=10,
                progress_bar=True
            )
            
            print("âœ… Training completed!")
            
        except Exception as e:
            print(f"âŒ Training failed: {e}")
    
    def predict(self, state: np.ndarray) -> Tuple[np.ndarray, Any]:
        """
        Predict action for given state
        
        Returns:
            action, _state
        """
        if self.model is None:
            # Fallback to heuristic
            return np.array([0.7, 0.25, 0.25, 0.25, 0.25], dtype=np.float32), None
        
        return self.model.predict(state, deterministic=True)
    
    def save(self, path: Optional[str] = None):
        """Save trained model"""
        save_path = path or self.model_path
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        if self.model is not None:
            self.model.save(save_path)
            print(f"ğŸ’¾ Model saved to {save_path}")
        
        # Save demonstrations
        demo_path = save_path + '_demonstrations.pkl'
        with open(demo_path, 'wb') as f:
            pickle.dump(self.demonstrations, f)
        print(f"ğŸ’¾ Demonstrations saved to {demo_path}")
    
    def load(self, path: Optional[str] = None):
        """Load trained model"""
        load_path = path or self.model_path
        
        try:
            from stable_baselines3 import SAC
            
            self.model = SAC.load(load_path, env=self.env)
            print(f"ğŸ“‚ Model loaded from {load_path}")
            
            # Load demonstrations
            demo_path = load_path + '_demonstrations.pkl'
            if os.path.exists(demo_path):
                with open(demo_path, 'rb') as f:
                    self.demonstrations = pickle.load(f)
                print(f"ğŸ“‚ Demonstrations loaded from {demo_path}")
            
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
    
    def evaluate(self, n_eval_episodes: int = 10) -> Dict[str, float]:
        """
        Evaluate agent performance
        
        Returns:
            Dictionary with evaluation metrics
        """
        if self.model is None:
            print("âš ï¸  No model to evaluate")
            return {}
        
        from stable_baselines3.common.evaluation import evaluate_policy
        
        mean_reward, std_reward = evaluate_policy(
            self.model,
            self.env,
            n_eval_episodes=n_eval_episodes,
            deterministic=True
        )
        
        metrics = {
            'mean_reward': mean_reward,
            'std_reward': std_reward,
            'n_episodes': n_eval_episodes
        }
        
        print(f"ğŸ“Š Evaluation: mean_reward={mean_reward:.2f} Â± {std_reward:.2f}")
        
        return metrics


def create_default_demonstrations() -> List[Dict]:
    """
    Create default demonstrations from heuristic parameters
    
    ë…¼ë¬¸ì—ì„œ GRAPE í„ìŠ¤ë¡œ demonstration ìƒì„±í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬
    """
    demonstrations = []
    
    # Demonstration 1: ê¸°ë³¸ íœ´ë¦¬ìŠ¤í‹±
    demonstrations.append({
        'name': 'default_heuristic',
        'action': np.array([0.7, 0.25, 0.25, 0.25, 0.25], dtype=np.float32),
        'description': 'ê¸°ë³¸ ì´ìƒíƒì§€ íŒŒë¼ë¯¸í„°'
    })
    
    # Demonstration 2: ë³´ìˆ˜ì  íŒë‹¨ (ë†’ì€ ì„ê³„ê°’)
    demonstrations.append({
        'name': 'conservative',
        'action': np.array([0.8, 0.3, 0.3, 0.2, 0.2], dtype=np.float32),
        'description': 'ë³´ìˆ˜ì  íŒë‹¨ (ìœ„ì‘ìœ¼ë¡œ íŒë‹¨í•˜ê¸° ì–´ë µê²Œ)'
    })
    
    # Demonstration 3: ê³µê²©ì  íŒë‹¨ (ë‚®ì€ ì„ê³„ê°’)
    demonstrations.append({
        'name': 'aggressive',
        'action': np.array([0.6, 0.2, 0.2, 0.3, 0.3], dtype=np.float32),
        'description': 'ê³µê²©ì  íŒë‹¨ (ìœ„ì‘ìœ¼ë¡œ íŒë‹¨í•˜ê¸° ì‰½ê²Œ)'
    })
    
    return demonstrations

